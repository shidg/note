# CentOS 7 + hadoop-2.7.5 + hbase-1.2.6
# 服务器数量4，主机名分别为K-master K-slave1 K-slave2 K-slave3

# on all servers
useradd hadoop

#配置主从服务器之间的双向SSH免密认证(hadoop用户)，包含K-master本机无密码登录、K-master无密码登录K-slave1~K-slave3、K-slave1~K-slave3无密码登录K-master
# K-slave1(备用主节点)本机无密码登录、无密码登录K-slave1~K-slave3、K-slave1~K-slave3无密码登录K-slave1



############# INSTALL HADOOP ON K-master ##################
tar zxvf hadoop-2.7.5.tar.gz -C /Data/app && cd /Data/app && ln -s hadoop-2.7.5 ./hadoop && mkdir hadoop/tmp && mkdir -p hadoop/hdfs/{name,data} && chown -R hadoop:hadoop hadoop-2.7.5

echo >> /etc/profile <<EOF
#JAVA
export JAVA_HOME=/Data/app/jdk1.8.0_162
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
#HADOOP
export HADOOP_HOME=/Data/app/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_HOME_WARN_SUPPRESS=1
#HBASE
export  PATH=$PATH:/Data/app/hbase/bin
EOF


# As hadoop users #

cd /Data/app/hadoop/etc/hadoop

## hadoop-env.sh ##
export JAVA_HOME=/Data/app/jdk1.8.0_162
export HADOOP_IDENT_STRING=$USER
export HADOOP_PREFIX=/Data/app/hadoop
export HADOOP_SSH_OPTS="-p 5122"


## core-site.xml ##
<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://K-master:9000</value>
    </property>
     <property>
        <name>hadoop.tmp.dir</name>
        <value>/Data/app/hadoop/tmp</value>
    </property>
    <property>
        <name>fs.checkpoint.period</name>
        <value>3600</value>
    <description>
        The number of seconds between two periodic checkpoints.
    </description>
    </property>
    <property>
        <name>fs.checkpoint.size</name>
    <value>67108864</value>
    </property>
</configuration>


## hdfs-site.xml ##
<configuration>
 <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.name.dir</name>
        <value>/Data/app/hadoop/hdfs/name</value>
    </property>
    <property>
        <name>dfs.data.dir</name>
        <value>/Data/app/hadoop/hdfs/data</value>
    </property>
    <property>
        <name>dfs.http.address</name>
        <value>K-master:50070</value>
    <description>
        The address and the base port where the dfs namenode web ui will listen on.
        If the port is 0 then the server will start on a free port.
    </description>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>K-slave2:50090</value>
    </property>
</configuration>


## mapred-env.sh ##
export JAVA_HOME=/Data/app/jdk1.8.0_162


## mapred-site.xml ##
<configuration>
        <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
        <description>Execution framework set to Hadoop YARN.</description>
        </property>
</configuration>


## yarn-site.xml ##
<configuration>
        <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>K-master</value>
        </property>
        <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        </property>
</configuration>

## cat  masters ## #
## 这个master文件的作用是指定SecondaryNameNode的安装位置，可是是多台，但是最好不要与NameNode在同一台机器
K-slave2

## cat slaves ##
K-slave1
K-slave2
K-slave3







################################## INSTALL HADOOP ON K-slave1~K-slave3 ########################################

#过程、所有配置均与K-master完全相同，唯一不同的是K-slave上slaves文件为空，或可直接删除slaves文件







########################################### STAAR HADOOP (ON K-master)######################################

#格式化hdfs
hdfs namenode -format

#启动hadoop
start-dfs.sh

#启动分布式计算
start-yarn.sh


# 查看进程
jps

K-master上为NameNode、SecondaryNameNode、ResourceManager，其他三台为DataNode、NodeManager








#################################### INSTALL HBASE ON K-master ################

tar zxvf hbase-1.2.6-bin.tar.gz -C /Data/app && cd /Data/app && chown -R hadoop:hadoop hbase-1.2.6 && ln -s hbase-1.2.6 ./hbase

cd hbase/conf

## hbase-env.sh ##
export JAVA_HOME=/Data/app/jdk1.8.0_162
export HBASE_CLASSPATH=/Data/app/hadoop/etc/hadoop
export HBASE_MANAGES_ZK=false
export HBASE_HOME=/Data/app/hbase
export HADOOP_HOME=/Data/app/hadoop
export HBASE_LOG_DIR=/Data/app/hbase/logs    #Hbase日志目录
export HBASE_SSH_OPTS="-p 5122


## hbase-site.xml ##

<configuration>
   <property>
       <name>hbase.rootdir</name>
       <value>hdfs://K-master:9000/hbase</value>
   </property>
   <property>
       <name>hbase.cluster.distributed</name>
       <value>true</value>
   </property>
   <!-- 若不指定hbase.master.port，在启动backup-master的时候会报addressAlready in use的错误-->
   <property>
      <name>hbase.master.port</name>
      <value>16000</value>
   </property>
   <!--hbase.zookeeper.quorum的个数为奇数 -->
   <property>
       <name>hbase.zookeeper.quorum</name>
       <value>K-slave1,K-slave2,K-slave3</value>
   </property>
</configuration>

## cat regionservers ##
K-slave-1
K-slave-2
K-slave-3

## cat backup-masters ##
K-slave1






################################### INSTALL HBASE ONK-slave1~K-slave3##############
# 过程、所有配置均与K-master完全相同，唯一不同的是K-slave上没有backup-masters文件
# 另外，如果K-slave1作为backup-master，那么要保证K-slave1可以无密码ssh到其他任何一台机器，其他所有机器也都可以无密码ssh到K-slave1(道理同K-master)



## ########################################INSTALL ZOOKEEPER ON K-slave1~3##################################

echo >> /etc/profile << EOF
# for zookeeper
JAVA_HOME=/Data/app/jdk1.8.0_162
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/jre/lib
ZOOKEEPER_HOME=/Data/app/zookeeper-3.4.11
PATH=$PATH:$JAVA_HOME/bin:$ZOOKEEPER_HOME/bin
export JAVA_HOME CLASSPATH ZOOKEEPER_HOME PATH
EOF

echo >> /etc/hosts <<EOF
10.X.X.X K-slave1
10.X.X.X K-slave2
10.X.X.X K-slave3
EOF

tar zxvf zookeeper-3.4.11.tar.gz -C /Data/app
cd /Data/app/zookeeper-3.4.11/conf
cp zoo_sample.cfg zoo.cfg

echo > zoo.cfg <<EOF
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/Data/zookeeper
clientPort=2181
#maxClientCnxns=60
#autopurge.snapRetainCount=3
#autopurge.purgeInterval=1
server.1=K-slave1:2888:3888
server.2=K-slave2:2888:3888
server.3=K-slave3:2888:3888
EOF

mkdir /Data/zookeeper
echo 1 > /Data/zookeeper/myid
#每个服务器上的myid文件中的值不得相同，用来标识不同的zookeeper节点


chown -R hadoop:hadoop /Data/app/zookeeper-3.4.11





################################################ STARTZOOKEEPER###########################################
su - hadoop
zkServer.sh start




####################################### START HBASE #####################
# 在K-master上
start-hbase.sh


#查看进程

jps

K-master上为HMaster,  K-slave1上为HMaster、HRegionServer、HQuorumPeer, K-slave2和K-slave3上为HRegionServer、HQuorumPeer



#服务启动

# 第一步: 启动hadoop
# 在K-master上,
./sbin/start-dfs.sh
./sbin/start-yarn.sh

#第二步，启动ZK
#在K-slave1~K-slave3上
#zkServer.sh start
#
#第三步，启动hbase
#在K-master上
#./bin/start-hbase.sh

