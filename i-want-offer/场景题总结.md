#### 处理过哪些问题?

##### Redis Cluster部署过程中的防火墙设置问题

所有节点的Redis服务部署完成并且启动成功，但是Redis cluster创建失败z

因为gossip协议使用服务端口+10000的端口号进行通信，需要开放这个端口

---

##### WebLogic调优

  **1. JVM 参数调优**

* 根据服务内存使用和GC情况，设置合适的堆大小、GC策略：

  ```shell
  -Xms2048m -Xmx2048m -XX:+UseG1GC -XX:MaxGCPauseMillis=200
  ```

* 使用 `jstat` 和 `jmap` 工具分析 FullGC 频率，减少频繁GC引发的线程阻塞；
* 调整 PermGen/Metaspace 参数，避免类加载过多导致崩溃。

2. **线程池与连接数配置**

* 配置合理的线程最大数 (`ThreadPoolSize`) 和数据库连接池 (`JDBC Connection Pool`) 大小；
* 根据业务并发量及数据库承压能力，动态调整最小/最大连接数、空闲时间等。

3. **Session 管理优化**

* 关闭不必要的 Session 持久化功能；
* 对于无状态请求启用 Stateless 模式，降低 Session 负载。

---

##### jvm参数调优不生效

背景介绍：k8s中运行spring boot应用，使用-Xms -Xmx配置的堆栈大小不生效，容器内的jvm模式使用宿主机内存的1/4,导致pod被OOMkill

现象：

    pod反复重启，直至CrashLoopBackOff

排查过程：

    检查pod内容器的request和limits配置

    检查容器内java应用的jvm配置

    查阅资料，发现jdk8-191之前，是不能感知到自己是在容器中运行的，所以对容器设置的资源限制，java程序不能感知，因此每个jvm会占用宿主机总内存的1/4

    检查本地jdk版本，发现为jdk8-121,问题原因定位

    修改jvm参数

```shell
java -XX:+UseContainerSupport -XX:MaxRAMPercentage=90.0 -XX:MinRAMPercentage=60.0 -jar register.jar
```

---

##### 内存泄漏导致的服务崩溃

###### 背景介绍

* 项目名称：用户画像标签平台（Spring Boot 微服务架构）
* 部署方式：Docker容器 + Kubernetes
* JVM配置：`-Xms512m -Xmx1024m`
* 系统稳定运行几个月后，某天突发服务频繁重启，接口访问报502错误

###### 异常现象

* 服务突然频繁被 K8s 重启（Liveness 探针失效）
* 监控平台报警：容器内存使用率达100%
* 查看日志发现：java.lang.OutOfMemoryError: Java heap space

###### 🔍 排查过程

    **查看日志，确认是OOM异常**

    容器日志、JVM GC 日志中都出现`OutOfMemoryError`

    Kubernetes Event 中也提示 OOMKilled

    **分析GC日志 & 内存快照**

    启用`-XX:+HeapDumpOnOutOfMemoryError`，分析 dump 文件（使用 MAT 工具）

    发现大量`ArrayList` 和 `HashMap` 占用内存，来源是某个用户数据缓存逻辑
   **定位问题代码**

    某模块为了提升性能，使用了本地缓存机制（`Map<String, UserProfile>`）

    没有设置缓存清理策略，导致缓存一直增长

    某次任务批量处理了百万级用户数据后，缓存暴涨，引发OOM

---

##### log4j漏洞修复

Log4j 漏洞，特别是著名的  **Log4Shell（CVE-2021-44228）** ，是一个影响广泛的远程代码执行（RCE）漏洞。这个漏洞利用了 Log4j 在日志记录中对 JNDI 请求的不安全处理

Log4Shell 是 Log4J 某些版本中存在的远程代码执行 (RCE) 漏洞，由阿里巴巴的安全研究人员于 2021 年 11 月 24 日发现，其通用漏洞披露标识符为 CVE-2021-44228。该漏洞影响 Apache Log4J 2 的 2.14.1 版本及更早版本；Log4J 2.15 及更高版本和 Apache Log4J 1 的所有版本不受影响。

Apache先后发布了四个补丁来修复改漏洞

修复方式：

###### **1. 升级 Log4j 到安全版本**

这是最推荐、最直接的方法：

* **Log4j 2.x 用户：**
  * 升级到 **2.17.1** 或以上版本（适用于 Java 8 及以上）。
  * 对于 Java 7 用户，升级到  **2.12.4** 。
  * 对于 Java 6 用户，升级到  **2.3.2** 。

这些版本已经完全禁用了不安全的 JNDI 功能。

###### **2. 临时缓解措施（适用于不能立即升级的情况）**

如果短期内无法升级，可以使用以下方法降低风险：

   🔸删除 JndiLookup 类，禁用 JNDI 查找功能。

    在`log4j-core-*.jar` 中删除漏洞相关类：

```shell
 zip -q -d log4j-core-*.jar org/apache/logging/log4j/core/lookup/JndiLookup.class
```

🔸 禁用 JNDI 功能（Log4j >= 2.10）

设置 JVM 参数：

```shell
-Dlog4j2.formatMsgNoLookups=true
```

⚠️ 注意：此方法在 2.15 之后的版本中已默认禁用，因此不需要手动设置。

###### 🧰 **3. 检查是否使用 Log4j**

你可以通过以下方式确认是否存在 Log4j：

* 使用 `mvn dependency:tree` 查看依赖树（对于 Maven 项目）
* 使用 `find` 命令扫描 `.jar` 文件中是否包含 `JndiLookup.class`
* 运行 SCA（软件成分分析）工具，如：
  * **Dependabot** 、 **Snyk** 、 **WhiteSource** 、**JFrog Xray**

###### 🔒 **4. 加强服务器防护**

* 使用 WAF 拦截典型 payload，例如 `${jndi:ldap://...}`
* 限制出站网络访问，防止 Log4j 利用 JNDI 发起网络连接
* 关闭未使用的端口和服务

---

##### Cgroup驱动不一致导致的k8s集群不正常

k8s 1.22之后，默认的cgroup驱动是systemd，docker的默认cgroup驱动是cgroups，需要将驱动类型修改为一致

kubelet修改方式：

```
# /var/lib/kubelet/config.yaml
cgroupDriver: systemd
```

docker修改方式

```shell
# /etc/docker/daemon.json 
"exec-opts": ["native.cgroupdriver=systemd"]
```

---

##### 容器网段和物理机网段冲突

---

##### k8s证书过期、解决及预防

---

##### pod处PodCrashLoopBackOff状态的原因和解决

**问题表现**：

    pod无法启动，反复重启直到进入CrashLoopBakcOff状态

**原因分析** ：

* 镜像本身有问题，比如启动命令写错
* 程序依赖未就绪，比如数据库连接失败
* 容器内某个配置文件或环境变量出错
* 如果使用了 Init Container，有可能是init Container执行没有成功，确认其完成状态；

**排查与解决思路：**

    1. 验证镜像：在本地或测试环境单独运行该镜像，排除镜像自身问题

    2. 使用kubectl logs --previous查看Pod日志，通过日志定位容器无法启动的具体原因；

---

##### Service  DNS 解析失败

**问题表现：**

    集群内应用无法通过 Service 名称互相访问，报`getaddrinfo: Name or service not known`。

**原因分析：**

* CoreDNS工作异常
* service对应的Endpoints列表为空
* 服务名书写错误

**排查与解决思路：**

1. 确认 CoreDNS Pod 正常：
   kubectl get pod -n kube-system | grep coredns，查看状态是否为Running
2. 查看 CoreDNS 日志，检查是否有转发或解析错误；
   kubectl logs -f  -n kube-system coredns-xxx，查看是否有报错
3. 确认 Service 对应的 Endpoints 是否正常：
   kubectl get endpoints  `<svc_name>`

---

##### 网络插件（CNI）故障

**问题表现:**

    新节点加入后 Pod 无法与其他节点上的 Pod 通信，或者跨节点网络延迟、丢包严重。

**原因分析：**

* 新节点的网络插件未正常运行
* 新节点的网络配置有问题
* 新节点防火墙阻断了VXLAN端口
* MTU设置不当

**排查与解决思路：**

1. 检查 CNI 插件状态（如 Calico、Flannel、Weave Net）：
   kubectl get pod  -n kube-system (calico)
   kubectl get pod -n kube-flannel(flannel)
2. 登录到节点，查看 CNI 插件日志（通常在 `/var/log` 或 `journalctl -u kubelet`）；
3. 使用 `ping`、`traceroute`、`calicoctl`（若使用 Calico）排查路由表和网络策略；
4. 确认防火墙（iptables、firewalld）未阻断 VXLAN/UDP 端口
   iptables -nL或者关闭防火墙快速定位问题
5. 如果是 MTU 问题，调整 CNI 的 MTU 配置，使其与底层网络一致。

---

##### PersistentVolume（PV）与 PersistentVolumeClaim（PVC）绑定失败

**问题表现:**

    PVC 一直处于`Pending`，无法绑定到任何 PV

**原因分析：**

* 没有满足pvc要求的pv
* storageclass工作不正常
* 外部存储设备故障

**排查与解决思路：**

1. 查看 PVC 描述，确认 namespace、StorageClass、accessModes、storageSize 等参数是否与现有 PV 匹配
2. 如果使用动态 Provisioner，检查对应的 CSI 插件 Pod 是否正常
3. 对于 NFS、iSCSI 等网络存储，验证网络连通性与挂载权限

---

##### 资源调度与节点压力

**问题表现:**

    新Pod调度失败，旧 Pod 频繁被驱逐（Evicted)或被 OOMKilled

**原因分析：**

* 调度策略配置错误导致无可用节点
* 节点压力导致的自动驱逐

**排查与解决思路:**

1. 查看pod的节点选择器、亲和反亲和、容忍度配置，确认不是配置错误导致的无节点可用
2. 查看节点资源使用情况
   kubectl  top node
3. 检查是否有 Pod 未设置 `requests`/`limits`，导致调度时无法精确评估；
4. 为关键服务设置合理的 `requests`（保证调度）与 `limits`（防止“资源霸占”）；

---

##### Pod 跨命名空间通信问题

**问题表现：**

    不同 Namespace 中的应用需要互相访问，使用 Service 名称却无法解析。

**原因分析：**

* 调用地址没有写完整
* svc对应的Pod内的服务运行异常
* 设置了网络策略

**排查与解决思路**

1. 检查请求的Service DNS 名称后是否添加了Namespace：`<service>.<namespace>.svc.cluster.local`
2. 绕过svc直接请求pod IP，排除是否svc到pod的转发存在问题
3. 检查是否对pod设置了 NetworkPolicy ，从而阻止了
4. 若希望简化调用，可在 Pod 的 `/etc/resolv.conf` 中添加 `search` 域（通过 ConfigMap 或 Pod Spec）
5. 或者在 Service 上设置 ExternalName，将跨命名空间访问映射为同命名空间的别名；

---

##### **Service不能及时察觉出现问题的pod副本**

**问题现象** ：

    某服务的 Pod 已经崩溃或卡死，但访问 Service 时仍然会路由到这个 Pod，导致部分用户访问失败

**原因分析** ：

* readinessProbe 配置不当，导致未完全就绪的Pod提前进入 Ready状态
* Liveness Probe 失败未能正确 kill 掉异常 Pod
* kube-proxy 使用的 iptables/ipvs 规则更新有延迟，导致endpoints列表没有及时更新
* kubelet对Pod的生命周期管理出现问题，不能及时修改pod的状态

 **排查与解决思路：**

* 调整探针设置，确保应用完全就绪之后再将pod添加到endpoints列表，尽快识别出“不健康”Pod，将其从endpoints列表剔除；
* 使用 `kubectl describe endpoints` 查看 Service 实际关联了哪些 Pod；
* 检查kube-proxy的工作状态
  kubectl get pod -n kube-system  | grep kube-proxy
* 检查是否 kubelet 卡顿或 node 有性能瓶颈

---

##### **Node 节点频繁 NotReady，Pod 被反复驱逐**

**问题现象** ：

    某个 node 节点状态频繁在`Ready` 和 `NotReady` 之间切换，Pod 被反复驱逐。

**原因分析** ：

* 节点压力导致的状态异常，比如磁盘或者innode用尽
* 节点kubelet服务不稳定
* 节点网络不稳定；

**排查与解决思路 ：**

    1. 用`df -h` / `df -i` / `top` 诊断节点的磁盘/内存/CPU

    2. 查看`/var/log/kubelet.log` 或 systemd 日志

    3. 查看节点的网络配置，使用ping、traceroute等命令检查节点的路由和网络延迟

    4. 必要时考虑给节点设置`taint`，或手动排空维护

---

##### 权限不足导致应用无法访问 Kubernetes API

**问题现象** ：

    应用通过 ServiceAccount 访问集群 API 报 403 Forbidden。

**原因分析** ：

* 没有给对应的 ServiceAccount 绑定正确的 Role 或 ClusterRole
* RBAC 策略缺失或写错

**处理方式** ：

    1. 查看pod日志或者k8s时间日志，确认是否有"RBAC deny"相关的报错

    2. 使用`kubectl auth can-i` 检查权限

3. 创建 RoleBinding/ClusterRoleBinding 授权

---

#### 有过哪些失误？

##### 资源限制配置不当导致 Pod 被 OOMKilled 或 CPU Throttling

**问题现象** ：

* Pod 容器频繁被杀掉，状态为 `OOMKilled`
* 应用性能极差，实际 CPU 使用远低于请求值

**原因分析** ：

* `resources.requests/limits` 设置不合理
* 内存不足或 CPU 限额太低导致容器被频繁限制

**处理方式** ：

* 查看容器重启状态 `kubectl get pod -o wide`；
* 使用 `kubectl top pod` 或 Prometheus 监控查看资源使用；
* 调整 limits 并考虑使用 HPA 自动扩缩容；

---

##### sudo 权限设置不当导致业务中断

**问题现象** ：

    开发在某次 sudo 操作中误删系统目录；

**原因分析** ：

* 未细化 sudo 权限控制；
* 运维脚本未加安全检查逻辑；

**处理方式** ：

* 使用 `/etc/sudoers.d/` 为用户分组做精准授权；
* 禁止执行危险命令（如 rm -rf /）；
* 上线前做脚本 dry-run +审计回溯（auditd）；
* 高权限命令通过跳板机/堡垒机审核操作记录。

---

##### **NTP 同步异常导致系统时间漂移**

**问题现象** ：

    日志时间混乱，某些程序认证失败（比如 JWT、SSL）；

**原因分析** ：

* 主机未正确配置 NTP；
* 被墙或者 NTP 源不稳定；

**处理方式** ：

* 配置内网可信 NTP 服务源；
* 使用 `chronyd` 替代 `ntpd`，支持瞬时时间漂移调整；
* 加入定时同步任务，或加入 Zabbix 等告警监控时间差。

---

##### rsync/ansible 批量操作误伤系统目录

**问题现象** ：

    运维批量下发时不小心删除了线上配置或代码；

**原因分析** ：

* rsync 使用 `--delete` 忘记加路径末尾 `/`；
* Ansible playbook 写法不严谨；

**处理方式** ：

* 所有批量操作前加 `--dry-run`；
* 在生产环境启用只读保护（immutable）；
* 配合 Git 或版本控制方案做全量备份；
* 对关键操作加入 `二次确认机制`（脚本交互式输入 yes）；

---

##### 系统日志暴增导致 CPU 飙高

**问题现象** ：

    某服务 log level 设置为 debug，瞬间写爆磁盘和 CPU；

**处理方式** ：

* 降低日志级别到 `INFO`；
* 配置 `rsyslog` 限流；
* 设置 journald 日志轮转参数，限制最大体积 `SystemMaxUse`；
* 开启日志采集系统（如 filebeat + ELK），结合告警监控。

---

##### 权限设置过宽导致开发误删数据

**背景** ：

    为方便调试，把数据库测试账号直接授予了线上权限（偷懒行为）；

**失误**：

    开发误删了一批真实用户数据，导致部分用户无法正常使用

**造成的影响：**

    虽然后来恢复了备份，但流程不合规

**补救措施** ：

* 开始使用基于最小权限原则的 RBAC；
* 数据库做了主从只读隔离；
* 核心数据引入审计和回滚机制（binlog + PITR 恢复）；

**经验/教训 ：**

    一次小事故让我意识到权限控制必须“上限死守”，安全意识从那刻起提升了。

---

##### 误操作导致业务服务短暂中断

**背景** ：

    早期没有搭建 CI/CD 平台，我用手动方式更新一台测试环境的配置文件；

**失误** ：误操作导致把 `nginx.conf` 推到了线上环境，重载时服务瞬间崩了；

**后果** ：前端页面 502，持续了不到 1 分钟，但用户已经能感知到；

**补救措施** ：

* 立刻回滚配置；
* 加入运维操作审批和灰度机制；
* 推动上线 GitOps 机制，所有配置都纳入版本控制 + 自动部署；

**经验/教训** ：理解了“手动=事故”，从此形成规范化上线流程，极大减少人工失误。

---

#### 接到过哪些报警，怎么处理的？

##### dns解析失败导致的外部接口调用失败

**背景介绍：**

    房山区长沟镇网上政务厅平台中某业务模块需要调用第三方OCR（文字识别）接口，从用于上传的身份证图片中提取用户信息，该政务平台运行在公有云

**故障描述：**

    收到监控系统报警，用户注册接口响应超时（Timeout）

**影响范围：**

    用户注册时，身份证照片上传后识别失败，只能手动填写身份证号码等用户信息，虽然业务最终能完成，但是用户体验受到影响。

**处理过程：**

* 根据报警信息初步定位是业务中的文字识别服务出现问题
* 与研发人员确认，文字识别服务是通过调用第三方OCR接口实现
* 登录服务器查看文字识别服务相关服务的运行日志，根据OCR接口地址进行过滤，发现调用该接口超时
* 使用ping分别排查本地和第三方接口的网络连通性，确认网络层正常
* 使用nslookup排查域名解析，发现域名解析失败
* 手动更换dns服务器之后，域名解析恢复正常，确认是云主机的默认DNS有问题
* 提工单反馈问题，公有云排查后发现是该可用区的其中一台dns服务器异常，属于云服务异常导致的故障
* 修改DNS临时解决问题，等待云服务商解决故障
* 总结故障报告，向客户说明情况

---

##### 凌晨磁盘告警，日志异常增长导致服务即将中断

凌晨 01:42，接到监控告警，提示某生产节点磁盘使用率超过 90%

Step 1：登录主机，确认磁盘占用

Step 2：查看异常日志增长的服务

Step 3：止血措施

* 切割日志（临时）：
  mv user-api.log user-api.log.bak

  systemctl restart user-api
* **清理旧日志：**

  find /var/log/app-logs/ -name "*.log.*" -mtime +7 -delete
* **增加日志轮转配置：**

  * 修改 `logrotate` 配置，调整为每天轮转、最多保留 5 个文件
  * 限制单个日志大小为 100MB
* **服务层面限制日志级别：**

  * 临时将 `ERROR` 改为 `WARN`，减少无意义的重复输出

Step 4：根因分析

* 问题源于用户服务代码在处理第三方回调时，未做空值判断，触发空指针异常
* 异常进入死循环，每秒打印大量栈信息，导致日志飙涨
* 该模块部署在多个节点，另有两台服务器磁盘使用率也在上升

Step 5：优化与复盘

* 由开发团队修复空指针 bug，并上线热修复版本
* 推出统一日志采集方案 + 异常关键字报警
* 设置磁盘使用预警为 80%，提前感知风险
* 所有服务启用 logrotate + 日志级别动态调控能力

结果与反思：

* 本次故障在凌晨 2:20 前得到控制，避免了生产节点磁盘爆满导致服务写入失败
* 没有造成用户实际影响，但暴露出日志控制不严格的问题
* 后续推动统一日志管理平台（ELK + Filebeat）
*

---

##### 防火墙错误导致的接口调用超时

iptables规则调整失误

安全组规则调整失误

#### 解决问题的能力

##### 线上故障如何排查？

1 快速定位原因 ，怎么引起的，是否做了版本更新或者有人修改了配置？

2 定位不到的话再开始按照业务逻辑排查

---

##### 网站访问不到如何排查
